\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[margin=1.9in]{geometry}
\usepackage[document]{ragged2e}
\usepackage{listings}
\usepackage{setspace}

\linespread{1.3}

\begin{document}

    \begin{center}
    \end{center}
    
    \addvspace{20mm}
        
    \begin{center}
        \huge Novel use of a Functional HDL to Simplify Development of an RNG Brute-Force Algorithm
    \end{center}
    
    \begin{center}
    \end{center}
       
    \begin{center}
        \large Andreas Stocker
    \end{center}
    
    \begin{center}
        \small \emph {University of Nicosia}
    \end{center}

    \addvspace{15mm}

    \section*{Abstract}

    Todo add abstract....
    
    TODO - add results/conclusions after project is done

    \section{Introduction}

    TODO

    \break

    \section{Introduction to FPGAs}

    The sophisticated FPGAs of today are the product numerous incremental improvements
    over the years [3]. One such step in the evolution are Programmable Read Only Memories
    also known as PROMs. These proms were used to implement logic gates. There are also
    different varieties of PROMs where some of them can only be programmed once and
    others which could be reprogrammed multiple times. PROMs had a drawback in that
    sequential logic could not be completely encapsulated within a PROM and would need
    to be added to a circuit as separate components. Another glaring drawback of PROMs was
    their lack of speed.

    Programmable Logic Arrays also known as PLAs made significat improvements over PROMs [3].
    Namely, PLAs were generally must faster that PROMs. They could also support a far
    larger number of inputs. Though one drawback was that number of combinations of
    logic elements was slightly more constrained than that of of PROMs.

    Programmable Array Logic (not to be confused with Programmable Logic Arrays) were the next
    step in the evolution that would culminate in the FPGAs of today [3]. Programmable Array Logic
    also known as PALS added support for clock elements as well as flip flops. They
    were much more sophisticated in their support for expressing sequential logic. Futhermore,
    they had the benefit of great performance.

    The FPGA was designed with the goal of accomplishing the same computations as ASICs but
    with the added benefit of reprogrammability. Because of this, FPGAs are frequently
    used to emulate ASICs, as well as to act as temporariy stand-ins while ASICs
    are still being produced. FPGAs as they are today have benefitted greatly
    from advances in CMOS design that were initially made with improving CPUs in mind.
    However, it was precisely because CPUs were becoming so efficient that custom hardware
    lose a good amount of popularity [6]. It became easier for companies to simply using
    general purpose CPUs instead of investing in tailor-made hardware level designs.
    One example of such a use-case where general purpose CPUs won is databases [6].
    This was in the late 70's and researchers were trying to create a "databse-machine".
    It was specifically tailored to run database queries on low-level hardware.

    When it comes to the benefits of using FPGAs verses CPUs there are several factors to keep in mind.
    Image and signal processing are two applications where FPGAs shine [6].
    Partially this is because FPGAs can generally offer a greater level of determinism
    that CPUs. In an FPGA latecy for some tasks can often not only be lower, but the latecy
    and also be predictable. This is crucial for applications where real-time
    responsiveness is key. Another place where FPGAs shine is parallelism [6].
    CPUs main unit of parallelism is their cores. FPGAs however, have a far more granular
    unit of parallelism which is their logic blocks. This allows for orders of magnitude
    more potential parallelism over CPUs. These factors make FPGAs ideal candidates
    for high-throughput low-latecy applications.

    When in comes to solving real-world problems, FPGAs are quickly moving out of niche,
    highly-specialed projects and are becoming more common in commondity setups [6].
    The main areas where FPGAs are gaining popularity are both in the networking sector
    as well as the graphics processing sector. One reason for this is that the amount
    of data generated in the world is growing rapidly. For this reason, the raw processing
    power of FPGAs is expected to become indispensable for many more companies in the future.

    One concrete use case for FPGAs is database co-processing [6]. This is because
    streaming databases are required to process data with a low latecy even under heavy
    load. Ironically, this is one area that is similar to the "database-machine" style
    projects of the 70's. So perhaps FPGA designers will run into the same sort of
    problems that "database-machine" researchers encountered [6] all those years ago.
    Undianiably though, there is a large room for improvement in this sector.
    One reason FPGAs are still somewhat niche is the fact that FPGA design is not
    very accessible to "mainstream" programmers. This is because "mainstream" programmers
    are familiar with the so-called Von Neumann architecture. The Von Neumann architecture
    assumes instructions are executed from memory and run in a given order. Neither of these
    factors apply when programming FPGAs.

    There are several projects which aim to overcome the reduced ease of use of FPGAs
    for mainstream programmers. The "Kiwi" and "Liquid Metal" projects aim to do exactly
    this [6]. The goal is that FPGAs be used with general purpose languages.
    However the general sentiment in the FPGA developer community seems to be that these
    projects that allow mainstream languages be used with FPGAs are not developed
    enough to be used for mission-critical applications.
    It could be that at the most fundamental level FPGAs are incompatible with languages
    designed for Von Neumann runtimes.
    Another way in which mainstream appeal for FPGAs can be improved is by
    providing developers with out of the box IP that developers can setup on FPGAs to interface
    with projects running on CPUs programmed in mainstream languages like C.

    Another point of comparison between FPGAs and CPUs is clock speed [6]. FPGAs need to operate
    at significatly lower clock speeds than CPUs. Though the amount of work done by FPGAs
    in a single clock cycle can sometimes be orders of magnitude greater in FPGAs. This benefits
    FPGAs by reducing the amount of power consumed by the device relative to a CPU. One downside
    is that the FPGA may be able to do less computations overall due to this slower speed.

    Yet another noteworthy point of comparison between FPGAs and CPUs is their memory model.
    CPUs with their Von Neumann architecture, generally have memory on a separate chip
    than the CPU. Though even if memory is technically on the same die as the CPU, like in Apple's
    M1 architecture, CPUs will always suffer from what is called the "Von Neumann bottleneck".
    Also termed the "memory wall" occurs because the entire CPU can only access one area
    of memory at once. One thing that mitigates this disadvantage is the ability to
    access a continous block on memory at once. This advantage does not exist if the memory
    is non in a continous block however. Memory is significatly different in FPGAs though.
    FPGAs have flip-flop registers as well as block ram [6]. Spare lookup tables can
    also be reprogrammed at runtime and used as additional memory. So in general, FPGA memory
    boasts far superior locality compared to CPU memory. It therefore has much better
    throughput, as well as lower latecy compared to the CPU.
    One nifty feature that exists because of the FPGA's memory model is known as
    "content addressible memory" (CAM). Content addressible memory can be used
    to implement a key value store with a constant lookup time.

    One thing modern CPUs have going for them when it comes to competing with FPGAs are
    SIMD operations [6]. This allows for parallel operations that mimic those of FPGAs.
    This means an operation (like addition for example) can be performed on two fixed
    sized arrays of values.

    \subsection{Open Source and FPGAs}

    When it comes to the development workflow, there is one major place where mainstream CPU programming
    is ahead of the tools used to develop for FPGAs. That is the arena of open source tools.
    Open source tools have grown to a point today where virtually all major tech companies
    either use open source or even contribute to it. These include Google, Apple and Amazon
    just to name a few.
    There are a number of benefits to having an open source ecosystem. The first major
    benefit is accessibility. If anyone can install a tool for free this is great
    for students and anyone who wants to learn about the project.
    Another benefit of open source is reliability. If a vendor simply stops updating a closed
    source project then everyone dependant an it is in serious trouble if bugs
    crop up and there is no one to fix them.
    Another factor is security. The more experienced people read and understand a piece of
    code the more likely it is that issues can be discovered.
    Open source is also great if the consumers of a project want to add their own features.
    There are some upsides to closed source software though. Closed source software with
    licensing fees means that a company can pay experienced developers to work on the project full
    time. This means the project is not dependant on people working on it on their free time.
    Of course, this does not apply to all open source projects as some projects are popular
    enough to have paid full time developers working on them.
    Also, when in comes to security through obscurity, the argument can be made that
    if attacker do not have access to the source code of a project, they can't find
    exploits for it as easily.

    The current state of open source tools for FPGA designs is as follows:
    project Icestorm provides decently feature-rich tools for a small number of FPGA
    models. Not only does Icestorm provide most features needed throughout the FPGA
    design workflow, it is even superious to proprietary tools in some ways. Icestorm
    does not suffer from the massive bloat and slowness of conventional FPGA tools.
    Not only are the tools included in Icestorm free from bloat, they can also be
    a lot faster some times.

    The downside of relying on the open source Icestorm project is that it largely depends
    on people investing their free time, and if these people lose interest in a particular
    tool that is part of the project, the entire workflow that uses this project
    may become untenable.

    \break
        
    \section{FPGA Architecture}

    An FPGAs internal architecture affects it's speed, area efficiency, and power consumption [1].
    FPGAs compete with ASICs in that they both allow a digital circuit to be created.
    They also have several advantages over ASICs because they can be reprogrammed in seconds
    and cost orders of magnitude less. ASICs can cost millions to produce but they do have
    their own benefits. FPGAs pay the price for their easy reconfigurability in area, delay,
    and power usage. ASICs are 20-35 more compact than FPGAs and require 10 times less power.
    This is because the FPGAs programmable routing circuitry causes overhead. Nevertheless,
    despite these downsides, FPGAs are far more viable for small to medium sized companies
    that can't spend millions on ASICs. ASICs cost millions because of three main factors.
    First, ASICs need expensive software in order for their circuits to be designed.
    Second, ASICs need a mask so their circuitry can be entched into silicon.
    The cost of this mask can be reduced by multiple different ASICs sharing a mask.
    Finally, hiring engineers to design a complex ASIC over multiple years is also quite
    expensive. These engineers cannot make even a small mistake in their design
    because that would ruin a mask which costs millions to produce.

    FPGAs suffer from no such complications because they can instantly be reprogrammed.
    In fact, FPGAs are often used by ASIC engineers to prototype and test their designs.

    Then it comes to their internal architecture, FPGAs consist of a variety of different
    components [1]. These include logic, memory, and multiplier blocks. All around these
    blocks is a programmable routing fabric that allows blocks to be configured to both
    communicate with each other as well as with the outside world through inputs and outputs.

    When it comes to memory, modern FPGAs use either flash, static memory, or anti-fuses [1].
    SRAM, a type of static memory, is very common in modern FPGAs like from the manufacturers
    Xilinx, Lattice and Altera. These SRAM cells perform two main roles in their FPGA's
    architectures. First, a majority of SRAM is used to configure interconnect signals.
    Most of the left over SRAM is used to persist information in lookup tables also known as
    LUTs.

    SRAM is used frequencly in modern FPGAs because it has a number of advantages [1]. SRAM
    does not have a limit to how many times it can be reprogrammed. This is unlike the
    EPROM of early FPGA which could only be reprogrammed once or a couple hundred times.
    Second, SRAM does not require any special electrical components and can be etched
    in silicon with CMOS techniques. These CMOS techniques allow FPGAs to benefit from
    all the production advancements made for modern CPUs.

    SRAM is not without its drawbacks howerver. Every SRAM cell needs 5-6 transistors
    which is quite a lot. Second, SRAM cannot persist information if a system is powered
    off. This necessitates the need for another system of persistent storage which increases
    the complexity of the system. Persistent storage is often provided by flash or EEPROM.
    Finally, storing a design in a persistent storage system like flash or EEPROM also
    makes it far easier to competitors of a company to reverse engineer a design.
    To mitigate this risk encryption is used.

    An alternative to SRAM is the so called floating gate technology [1]. This is used
    in flash or EEPROM which do not lose the data stored in them when power is lost.
    This flash based setup offers a number of benefits besides persistence.
    Persistence removes the need for a seperate storage mechanism. Persistence also
    makes it possible to run the device immediately after it starts up because there
    is no need for the programming step. This is important for certain use cases.
    Additionally, flash boasts greater area-efficiency compared to SRAM.

    One disadvantage brought by flash is that the floating gate requires that
    charge injection needs to be prevented. 
    
    \subsection{Lookup Tables}

    TODO
    
    \subsection{Flipflops}

    TODO
    
    \subsection{Routing}

    TODO
    
    \subsection{Memory}

    TODO

    \section{Related Work}

    Todo - add this

    THIS COMES NEAR THE END

    \break
    \section*{References}

    \begin{enumerate}

    \item Chandrasekaran, Shrutisagar, and Abbes Amira. "High performance FPGA implementation of the Mersenne Twister." 4th IEEE International Symposium on Electronic Design, Test and Applications (delta 2008). IEEE, 2008.

    \end{enumerate}
    
\end{document}
