\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[margin=1.9in]{geometry}
\usepackage[document]{ragged2e}
\usepackage{listings}
\usepackage{setspace}

\linespread{1.3}

\begin{document}

    \begin{center}
    \end{center}
    
    \addvspace{20mm}
        
    \begin{center}
        \huge Novel use of a Functional HDL to Simplify Development of an RNG Brute-Force Algorithm
    \end{center}
    
    \begin{center}
    \end{center}
       
    \begin{center}
        \large Andreas Stocker
    \end{center}
    
    \begin{center}
        \small \emph {University of Nicosia}
    \end{center}

    \addvspace{15mm}

    \section*{Abstract}

    Todo add abstract....
    
    TODO - add results/conclusions after project is done

    \section{Introduction}

    TODO

    \break

    \section{Introduction to FPGAs}

    The sophisticated FPGAs of today are the product numerous incremental improvements
    over the years [3]. One such step in the evolution are Programmable Read Only Memories
    also known as PROMs. These proms were used to implement logic gates. There are also
    different varieties of PROMs where some of them can only be programmed once and
    others which could be reprogrammed multiple times. PROMs had a drawback in that
    sequential logic could not be completely encapsulated within a PROM and would need
    to be added to a circuit as separate components. Another glaring drawback of PROMs was
    their lack of speed.

    Programmable Logic Arrays also known as PLAs made significat improvements over PROMs [3].
    Namely, PLAs were generally must faster that PROMs. They could also support a far
    larger number of inputs. Though one drawback was that number of combinations of
    logic elements was slightly more constrained than that of of PROMs.

    Programmable Array Logic (not to be confused with Programmable Logic Arrays) were the next
    step in the evolution that would culminate in the FPGAs of today [3]. Programmable Array Logic
    also known as PALS added support for clock elements as well as flip flops. They
    were much more sophisticated in their support for expressing sequential logic. Futhermore,
    they had the benefit of great performance.

    The FPGA was designed with the goal of accomplishing the same computations as ASICs but
    with the added benefit of reprogrammability. Because of this, FPGAs are frequently
    used to emulate ASICs, as well as to act as temporariy stand-ins while ASICs
    are still being produced. FPGAs as they are today have benefitted greatly
    from advances in CMOS design that were initially made with improving CPUs in mind.
    However, it was precisely because CPUs were becoming so efficient that custom hardware
    lose a good amount of popularity [6]. It became easier for companies to simply using
    general purpose CPUs instead of investing in tailor-made hardware level designs.
    One example of such a use-case where general purpose CPUs won is databases [6].
    This was in the late 70's and researchers were trying to create a "databse-machine".
    It was specifically tailored to run database queries on low-level hardware.

    When it comes to the benefits of using FPGAs verses CPUs there are several factors to keep in mind.
    Image and signal processing are two applications where FPGAs shine [6].
    Partially this is because FPGAs can generally offer a greater level of determinism
    that CPUs. In an FPGA latecy for some tasks can often not only be lower, but the latecy
    and also be predictable. This is crucial for applications where real-time
    responsiveness is key. Another place where FPGAs shine is parallelism [6].
    CPUs main unit of parallelism is their cores. FPGAs however, have a far more granular
    unit of parallelism which is their logic blocks. This allows for orders of magnitude
    more potential parallelism over CPUs. These factors make FPGAs ideal candidates
    for high-throughput low-latecy applications.

    When in comes to solving real-world problems, FPGAs are quickly moving out of niche,
    highly-specialed projects and are becoming more common in commondity setups [6].
    The main areas where FPGAs are gaining popularity are both in the networking sector
    as well as the graphics processing sector. One reason for this is that the amount
    of data generated in the world is growing rapidly. For this reason, the raw processing
    power of FPGAs is expected to become indispensable for many more companies in the future.

    One concrete use case for FPGAs is database co-processing [6]. This is because
    streaming databases are required to process data with a low latecy even under heavy
    load. Ironically, this is one area that is similar to the "database-machine" style
    projects of the 70's. So perhaps FPGA designers will run into the same sort of
    problems that "database-machine" researchers encountered [6] all those years ago.
    Undianiably though, there is a large room for improvement in this sector.
    One reason FPGAs are still somewhat niche is the fact that FPGA design is not
    very accessible to "mainstream" programmers. This is because "mainstream" programmers
    are familiar with the so-called Von Neumann architecture. The Von Neumann architecture
    assumes instructions are executed from memory and run in a given order. Neither of these
    factors apply when programming FPGAs.

    There are several projects which aim to overcome the reduced ease of use of FPGAs
    for mainstream programmers. The "Kiwi" and "Liquid Metal" projects aim to do exactly
    this [6]. The goal is that FPGAs be used with general purpose languages.
    However the general sentiment in the FPGA developer community seems to be that these
    projects that allow mainstream languages be used with FPGAs are not developed
    enough to be used for mission-critical applications.
    It could be that at the most fundamental level FPGAs are incompatible with languages
    designed for Von Neumann runtimes.
    Another way in which mainstream appeal for FPGAs can be improved is by
    providing developers with out of the box IP that developers can setup on FPGAs to interface
    with projects running on CPUs programmed in mainstream languages like C.

    Another point of comparison between FPGAs and CPUs is clock speed [6]. FPGAs need to operate
    at significatly lower clock speeds than CPUs. Though the amount of work done by FPGAs
    in a single clock cycle can sometimes be orders of magnitude greater in FPGAs. This benefits
    FPGAs by reducing the amount of power consumed by the device relative to a CPU. One downside
    is that the FPGA may be able to do less computations overall due to this slower speed.

    Yet another noteworthy point of comparison between FPGAs and CPUs is their memory model.
    CPUs with their Von Neumann architecture, generally have memory on a separate chip
    than the CPU. Though even if memory is technically on the same die as the CPU, like in Apple's
    M1 architecture, CPUs will always suffer from what is called the "Von Neumann bottleneck".
    Also termed the "memory wall" occurs because the entire CPU can only access one area
    of memory at once. One thing that mitigates this disadvantage is the ability to
    access a continous block on memory at once. This advantage does not exist if the memory
    is non in a continous block however. Memory is significatly different in FPGAs though.
    FPGAs have flip-flop registers as well as block ram [6]. Spare lookup tables can
    also be reprogrammed at runtime and used as additional memory. So in general, FPGA memory
    boasts far superior locality compared to CPU memory. It therefore has much better
    throughput, as well as lower latecy compared to the CPU.
    One nifty feature that exists because of the FPGA's memory model is known as
    "content addressible memory" (CAM). Content addressible memory can be used
    to implement a key value store with a constant lookup time.

    One thing modern CPUs have going for them when it comes to competing with FPGAs are
    SIMD operations [6]. This allows for parallel operations that mimic those of FPGAs.
    This means an operation (like addition for example) can be performed on two fixed
    sized arrays of values.

    \subsection{Open Source and FPGAs}

    When it comes to the development workflow, there is one major place where mainstream CPU programming
    is ahead of the tools used to develop for FPGAs. That is the arena of open source tools.
    Open source tools have grown to a point today where virtually all major tech companies
    either use open source or even contribute to it. These include Google, Apple and Amazon
    just to name a few.
    There are a number of benefits to having an open source ecosystem. The first major
    benefit is accessibility. If anyone can install a tool for free this is great
    for students and anyone who wants to learn about the project.
    Another benefit of open source is reliability. If a vendor simply stops updating a closed
    source project then everyone dependant an it is in serious trouble if bugs
    crop up and there is no one to fix them.
    Another factor is security. The more experienced people read and understand a piece of
    code the more likely it is that issues can be discovered.
    Open source is also great if the consumers of a project want to add their own features.
    There are some upsides to closed source software though. Closed source software with
    licensing fees means that a company can pay experienced developers to work on the project full
    time. This means the project is not dependant on people working on it on their free time.
    Of course, this does not apply to all open source projects as some projects are popular
    enough to have paid full time developers working on them.
    Also, when in comes to security through obscurity, the argument can be made that
    if attacker do not have access to the source code of a project, they can't find
    exploits for it as easily.

    The current state of open source tools for FPGA designs is as follows:
    project Icestorm provides decently feature-rich tools for a small number of FPGA
    models. Not only does Icestorm provide most features needed throughout the FPGA
    design workflow, it is even superious to proprietary tools in some ways. Icestorm
    does not suffer from the massive bloat and slowness of conventional FPGA tools.
    Not only are the tools included in Icestorm free from bloat, they can also be
    a lot faster some times.

    The downside of relying on the open source Icestorm project is that it largely depends
    on people investing their free time, and if these people lose interest in a particular
    tool that is part of the project, the entire workflow that uses this project
    may become untenable.

    \break
        
    \section{FPGA Architecture}

    An FPGAs internal architecture affects it's speed, area efficiency, and power consumption [1].
    FPGAs compete with ASICs in that they both allow a digital circuit to be created.
    They also have several advantages over ASICs because they can be reprogrammed in seconds
    and cost orders of magnitude less. ASICs can cost millions to produce but they do have
    their own benefits. FPGAs pay the price for their easy reconfigurability in area, delay,
    and power usage. ASICs are 20-35 more compact than FPGAs and require 10 times less power.
    This is because the FPGAs programmable routing circuitry causes overhead. Nevertheless,
    despite these downsides, FPGAs are far more viable for small to medium sized companies
    that can't spend millions on ASICs. ASICs cost millions because of three main factors.
    First, ASICs need expensive software in order for their circuits to be designed.
    Second, ASICs need a mask so their circuitry can be entched into silicon.
    The cost of this mask can be reduced by multiple different ASICs sharing a mask.
    Finally, hiring engineers to design a complex ASIC over multiple years is also quite
    expensive. These engineers cannot make even a small mistake in their design
    because that would ruin a mask which costs millions to produce.

    FPGAs suffer from no such complications because they can instantly be reprogrammed.
    In fact, FPGAs are often used by ASIC engineers to prototype and test their designs.

    Then it comes to their internal architecture, FPGAs consist of a variety of different
    components [1]. These include logic, memory, and multiplier blocks. All around these
    blocks is a programmable routing fabric that allows blocks to be configured to both
    communicate with each other as well as with the outside world through inputs and outputs.

    When it comes to memory, modern FPGAs use either flash, static memory, or anti-fuses [1].
    SRAM, a type of static memory, is very common in modern FPGAs like from the manufacturers
    Xilinx, Lattice and Altera. These SRAM cells perform two main roles in their FPGA's
    architectures. First, a majority of SRAM is used to configure interconnect signals.
    Most of the left over SRAM is used to persist information in lookup tables also known as
    LUTs.

    \subsection{Memory Variants}

    SRAM is used frequencly in modern FPGAs because it has a number of advantages [1]. SRAM
    does not have a limit to how many times it can be reprogrammed. This is unlike the
    EPROM of early FPGA which could only be reprogrammed once or a couple hundred times.
    Second, SRAM does not require any special electrical components and can be etched
    in silicon with CMOS techniques. These CMOS techniques allow FPGAs to benefit from
    all the production advancements made for modern CPUs.

    SRAM is not without its drawbacks howerver. Every SRAM cell needs 5-6 transistors
    which is quite a lot. Second, SRAM cannot persist information if a system is powered
    off. This necessitates the need for another system of persistent storage which increases
    the complexity of the system. Persistent storage is often provided by flash or EEPROM.
    Finally, storing a design in a persistent storage system like flash or EEPROM also
    makes it far easier to competitors of a company to reverse engineer a design.
    To mitigate this risk encryption is used.

    An alternative to SRAM is the so called floating gate technology [1]. This is used
    in flash or EEPROM which do not lose the data stored in them when power is lost.
    This flash based setup offers a number of benefits besides persistence.
    Persistence removes the need for a seperate storage mechanism. Persistence also
    makes it possible to run the device immediately after it starts up because there
    is no need for the programming step. This is important for certain use cases.
    Additionally, flash boasts greater area-efficiency compared to SRAM.

    One disadvantage brought by flash is that the floating gate requires that
    charge injection needs to be prevented. Another more serious downside of flash
    is its limited lifespan because can only be reprogrammed a fixed number of times.
    This is in contrast to SRAM which can be reprogrammed a virtually infinite amount of times.
    Depending on the application, this can be a serious disadvantage or a non-issue.
    The "Actel ProASIC3" for example can only be reprogrammed 500 times.
    This definitely could be an issue at least for prototyping where dozens of reprogramming
    cycles can occur in a day. Another very serious issue with flash where SRAM
    is superior is in the production run. One previously mentioned benefit of SRAM
    is that it can be produced using standard CMOS techniques. This is not the case with
    flash. Flash requires specialized components can cannot be created by etching
    silicon like with CMOS.

    In modern FPGAs there is a trend towards using a combination of flash storage and SRAM.
    This has the benefit of allowing infinite reconfigurability but this comes at a price
    of greater area overhead.

    The final type of FPGA configuration systems that are common today are anti-fuses [1].
    The most glaring difference between anti fuses and flash and SRAM is that the former
    can only be programmed once. This of course makes it unsuitable for many FPGAs applications
    like ASIC prototyping where multiple configuration runs are a neccesity.
    The name anti-fuse comes from the idea that the FPGA fabric consists of a number of fuses
    that can be selectively "blown". Though a more accurate description of this process
    would be the idea a of connecting these "fuses". This connection occurs when high voltage
    is sent through the "fuse". Most modern anti-fuses are mental-to-metal based.

    While the fact that anti-fuses cannot be reprogrammed after the initial configuration,
    they do come with the benefit of greater area efficiency [1]. In metal-to-metal
    anti-fuses this is because no silicon area is expended to allow configurable connectivity.
    One thing that does consume a significant amount of area however is the need for
    programming transistors.

    Another benefit of anti-fuses is the ability to include a greater number of switches
    because of lower on resistances and parasitic capacitence. Also,
    another obvious benefit that is shared with flash base FPGAs is the ability to
    work instantly when powered on since the fabric cannot change once programmed.

    Yet another benefit shared with flash based systems is that the design
    of the FPGA is harder to reverse engineer because it is not stored anywhere except
    the logic configuration itself. The security of the FPGA also benefits from the
    fact that because the FPGA can only be programmed once a malicious actor
    cannot reprogram the FPGA if given access to the device.

    When it comes to the manufacturing process, however, anti fuse FPGAs cannot
    benefit from using a CMOS silicon etching process [1]. Even worse,
    the fabrication process of anti fuse FPGAs is beginning to run into scaling problems
    and is generally lagging behind the sophistication of modern CMOS.

    To summarize, all three of these memory systems have their own pros and cons and
    which memory system will be used needs to be considered in light of the application
    and its constraints and goals. SRAM (as well as the flash SRAM hybrid approach) can
    be considered to be dominant howerver. This can be attributed to its compatibility
    with the sophisticated CMOS manufacturing process.
    
    \subsection{Logic Blocks}

    The basic unit of computation and storage within an FPGA is known as a logic block [1].
    The smallest a single logic block can be is to take the form of a single transistor.
    Though logic blocks can be far larger than that and even take the form of an entire
    CPU in theory. The size of a logic block, however, is something that needs to be carefully
    balanced for maxium efficiency to be achieved.

    If a logic block is too small for example, it will suffer from area inefficiency because
    it will increase the need for programmable routing. Furthermore, performance, and
    power consumption will also be negatively impacted.

    On the other extreme, logic blocks that encapsulate a large amount of logic may
    be performant, but they nullify the benefits an FPGA provides in the form of programmable
    logic. In effect, the circuit will become less and less of an FPGA, and more of
    a standard ASIC.

    Selecting logic blocks according to their size and functionality is a key consideration
    that FPGA creators (as in people creating the FGPA itself, not a design for it) need
    to consider. There are three main factors that need to be considered when selecting logic
    block type and instance count: these are area, speed and power.

    Logic blocks in modern FGPAs fall into two main categories: normal logic blocks,
    and blocks that perform specialized computations, like addition for example.
    What specialized computations will be included need to be carefully considered
    because unused specialized logic will inevitably lead to wasted area.

    Despite FPGAs being considered a completely general purpose device,
    creators of the FPGAs will in practice need to consider what kind of designs
    will run on a target FPGA [1].

    There is one main tradeoff that needs to be considered here:
    how much functionality is encapsulated into every logic block and how many
    logic blocks total are needed.
    Increasing the functionality in a single logic block reduces to total
    number of logic blocks needed (up to a diminishing point that is).
    Though as the logic block size increases, the number of wires connecting
    it in the routing increases.

    Most modern industrial FPGAs use a heirarchical approach that uses
    clusters of LUTs and flip flops [1]. This helps control the granularity
    of logic blocks.
    This involves grouping basic logic components together and connecting
    them with a local interconnect.
    This approach is used instead of increasing LUT size.
    The benefit is that the needed routing only grows quadradically instead
    of exponentially.
    
    \subsection{Flipflops}

    TODO
    
    \subsection{Routing}

    TODO
    
    \subsection{Memory}

    TODO

    \section{FPGA Design Workflow}

    TODO

    \section{Related Work}

    Todo - add this

    THIS COMES NEAR THE END

    \break
    \section*{References}

    \begin{enumerate}

    \item Chandrasekaran, Shrutisagar, and Abbes Amira. "High performance FPGA implementation of the Mersenne Twister." 4th IEEE International Symposium on Electronic Design, Test and Applications (delta 2008). IEEE, 2008.

    \end{enumerate}
    
\end{document}
