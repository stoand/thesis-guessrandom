\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[margin=1.9in]{geometry}
\usepackage[document]{ragged2e}
\usepackage{listings}
\usepackage{setspace}

\linespread{1.3}

\begin{document}

    \begin{center}
    \end{center}
    
    \addvspace{20mm}
        
    \begin{center}
        \huge Novel use of a Functional HDL to Simplify Development of an RNG Brute-Force Algorithm
    \end{center}
    
    \begin{center}
    \end{center}
       
    \begin{center}
        \large Andreas Stocker
    \end{center}
    
    \begin{center}
        \small \emph {University of Nicosia}
    \end{center}

    \addvspace{15mm}

    \section*{Abstract}

    Todo add abstract....
    
    TODO - add results/conclusions after project is done

    \section{Introduction}

    TODO

    \break

    \section{Introduction to FPGAs}

    The sophisticated FPGAs of today are the product numerous incremental improvements
    over the years [3]. One such step in the evolution are Programmable Read Only Memories
    also known as PROMs. These proms were used to implement logic gates. There are also
    different varieties of PROMs where some of them can only be programmed once and
    others which could be reprogrammed multiple times. PROMs had a drawback in that
    sequential logic could not be completely encapsulated within a PROM and would need
    to be added to a circuit as separate components. Another glaring drawback of PROMs was
    their lack of speed.

    Programmable Logic Arrays also known as PLAs made significat improvements over PROMs [3].
    Namely, PLAs were generally must faster that PROMs. They could also support a far
    larger number of inputs. Though one drawback was that number of combinations of
    logic elements was slightly more constrained than that of of PROMs.

    Programmable Array Logic (not to be confused with Programmable Logic Arrays) were the next
    step in the evolution that would culminate in the FPGAs of today [3]. Programmable Array Logic
    also known as PALS added support for clock elements as well as flip flops. They
    were much more sophisticated in their support for expressing sequential logic. Futhermore,
    they had the benefit of great performance.

    The FPGA was designed with the goal of accomplishing the same computations as ASICs but
    with the added benefit of reprogrammability. Because of this, FPGAs are frequently
    used to emulate ASICs, as well as to act as temporariy stand-ins while ASICs
    are still being produced. FPGAs as they are today have benefitted greatly
    from advances in CMOS design that were initially made with improving CPUs in mind.
    However, it was precisely because CPUs were becoming so efficient that custom hardware
    lose a good amount of popularity [6]. It became easier for companies to simply using
    general purpose CPUs instead of investing in tailor-made hardware level designs.
    One example of such a use-case where general purpose CPUs won is databases [6].
    This was in the late 70's and researchers were trying to create a "databse-machine".
    It was specifically tailored to run database queries on low-level hardware.

    When it comes to the benefits of using FPGAs verses CPUs there are several factors to keep in mind.
    Image and signal processing are two applications where FPGAs shine [6].
    Partially this is because FPGAs can generally offer a greater level of determinism
    that CPUs. In an FPGA latecy for some tasks can often not only be lower, but the latecy
    and also be predictable. This is crucial for applications where real-time
    responsiveness is key. Another place where FPGAs shine is parallelism [6].
    CPUs main unit of parallelism is their cores. FPGAs however, have a far more granular
    unit of parallelism which is their logic blocks. This allows for orders of magnitude
    more potential parallelism over CPUs. These factors make FPGAs ideal candidates
    for high-throughput low-latecy applications.

    When in comes to solving real-world problems, FPGAs are quickly moving out of niche,
    highly-specialed projects and are becoming more common in commondity setups [6].
    The main areas where FPGAs are gaining popularity are both in the networking sector
    as well as the graphics processing sector. One reason for this is that the amount
    of data generated in the world is growing rapidly. For this reason, the raw processing
    power of FPGAs is expected to become indispensable for many more companies in the future.

    One concrete use case for FPGAs is database co-processing [6]. This is because
    streaming databases are required to process data with a low latecy even under heavy
    load. Ironically, this is one area that is similar to the "database-machine" style
    projects of the 70's. So perhaps FPGA designers will run into the same sort of
    problems that "database-machine" researchers encountered [6] all those years ago.
    Undianiably though, there is a large room for improvement in this sector.
    One reason FPGAs are still somewhat niche is the fact that FPGA design is not
    very accessible to "mainstream" programmers. This is because "mainstream" programmers
    are familiar with the so-called Von Neumann architecture. The Von Neumann architecture
    assumes instructions are executed from memory and run in a given order. Neither of these
    factors apply when programming FPGAs.

    There are several projects which aim to overcome the reduced ease of use of FPGAs
    for mainstream programmers. The "Kiwi" and "Liquid Metal" projects aim to do exactly
    this [6]. The goal is that FPGAs be used with general purpose languages.
    However the general sentiment in the FPGA developer community seems to be that these
    projects that allow mainstream languages be used with FPGAs are not developed
    enough to be used for mission-critical applications.
    It could be that at the most fundamental level FPGAs are incompatible with languages
    designed for Von Neumann runtimes.
    Another way in which mainstream appeal for FPGAs can be improved is by
    providing developers with out of the box IP that developers can setup on FPGAs to interface
    with projects running on CPUs programmed in mainstream languages like C.

    Another point of comparison between FPGAs and CPUs is clock speed [6]. FPGAs need to operate
    at significatly lower clock speeds than CPUs. Though the amount of work done by FPGAs
    in a single clock cycle can sometimes be orders of magnitude greater in FPGAs. This benefits
    FPGAs by reducing the amount of power consumed by the device relative to a CPU. One downside
    is that the FPGA may be able to do less computations overall due to this slower speed.

    Yet another noteworthy point of comparison between FPGAs and CPUs is their memory model.
    CPUs with their Von Neumann architecture, generally have memory on a separate chip
    than the CPU. Though even if memory is technically on the same die as the CPU, like in Apple's
    M1 architecture, CPUs will always suffer from what is called the "Von Neumann bottleneck".
    Also termed the "memory wall" occurs because the entire CPU can only access one area
    of memory at once. One thing that mitigates this disadvantage is the ability to
    access a continous block on memory at once. This advantage does not exist if the memory
    is non in a continous block however. Memory is significatly different in FPGAs though.
    FPGAs have flip-flop registers as well as block ram [6]. Spare lookup tables can
    also be reprogrammed at runtime and used as additional memory. So in general, FPGA memory
    boasts far superior locality compared to CPU memory. It therefore has much better
    throughput, as well as lower latecy compared to the CPU.
    One nifty feature that exists because of the FPGA's memory model is known as
    "content addressible memory" (CAM). Content addressible memory can be used
    to implement a key value store with a constant lookup time.

    One thing modern CPUs have going for them when it comes to competing with FPGAs are
    SIMD operations [6]. This allows for parallel operations that mimic those of FPGAs.
    This means an operation (like addition for example) can be performed on two fixed
    sized arrays of values.

    \subsection{Open Source and FPGAs}

    When it comes to the development workflow, there is one major place where mainstream CPU programming
    is ahead of the tools used to develop for FPGAs. That is the arena of open source tools.
    Open source tools have grown to a point today where virtually all major tech companies
    either use open source or even contribute to it. These include Google, Apple and Amazon
    just to name a few.
    There are a number of benefits to having an open source ecosystem. The first major
    benefit is accessibility. If anyone can install a tool for free this is great
    for students and anyone who wants to learn about the project.
    Another benefit of open source is reliability. If a vendor simply stops updating a closed
    source project then everyone dependant an it is in serious trouble if bugs
    crop up and there is no one to fix them.
    Another factor is security. The more experienced people read and understand a piece of
    code the more likely it is that issues can be discovered.
    Open source is also great if the consumers of a project want to add their own features.
    There are some upsides to closed source software though. Closed source software with
    licensing fees means that a company can pay experienced developers to work on the project full
    time. This means the project is not dependant on people working on it on their free time.
    Of course, this does not apply to all open source projects as some projects are popular
    enough to have paid full time developers working on them.
    Also, when in comes to security through obscurity, the argument can be made that
    if attacker do not have access to the source code of a project, they can't find
    exploits for it as easily.

    The current state of open source tools for FPGA designs is as follows:
    project Icestorm provides decently feature-rich tools for a small number of FPGA
    models. Not only does Icestorm provide most features needed throughout the FPGA
    design workflow, it is even superious to proprietary tools in some ways. Icestorm
    does not suffer from the massive bloat and slowness of conventional FPGA tools.
    Not only are the tools included in Icestorm free from bloat, they can also be
    a lot faster some times.

    The downside of relying on the open source Icestorm project is that it largely depends
    on people investing their free time, and if these people lose interest in a particular
    tool that is part of the project, the entire workflow that uses this project
    may become untenable.

    \break
        
    \section{FPGA Architecture}

    An FPGAs internal architecture affects it's speed, area efficiency, and power consumption [1].
    FPGAs compete with ASICs in that they both allow a digital circuit to be created.
    They also have several advantages over ASICs because they can be reprogrammed in seconds
    and cost orders of magnitude less. ASICs can cost millions to produce but they do have
    their own benefits. FPGAs pay the price for their easy reconfigurability in area, delay,
    and power usage. ASICs are 20-35 more compact than FPGAs and require 10 times less power.
    This is because the FPGAs programmable routing circuitry causes overhead. Nevertheless,
    despite these downsides, FPGAs are far more viable for small to medium sized companies
    that can't spend millions on ASICs. ASICs cost millions because of three main factors.
    First, ASICs need expensive software in order for their circuits to be designed.
    Second, ASICs need a mask so their circuitry can be entched into silicon.
    The cost of this mask can be reduced by multiple different ASICs sharing a mask.
    Finally, hiring engineers to design a complex ASIC over multiple years is also quite
    expensive. These engineers cannot make even a small mistake in their design
    because that would ruin a mask which costs millions to produce.

    FPGAs suffer from no such complications because they can instantly be reprogrammed.
    In fact, FPGAs are often used by ASIC engineers to prototype and test their designs.

    Then it comes to their internal architecture, FPGAs consist of a variety of different
    components [1]. These include logic, memory, and multiplier blocks. All around these
    blocks is a programmable routing fabric that allows blocks to be configured to both
    communicate with each other as well as with the outside world through inputs and outputs.

    When it comes to memory, modern FPGAs use either flash, static memory, or anti-fuses [1].
    SRAM, a type of static memory, is very common in modern FPGAs like from the manufacturers
    Xilinx, Lattice and Altera. These SRAM cells perform two main roles in their FPGA's
    architectures. First, a majority of SRAM is used to configure interconnect signals.
    Most of the left over SRAM is used to persist information in lookup tables also known as
    LUTs.

    \subsection{Memory Variants}

    SRAM is used frequencly in modern FPGAs because it has a number of advantages [1]. SRAM
    does not have a limit to how many times it can be reprogrammed. This is unlike the
    EPROM of early FPGA which could only be reprogrammed once or a couple hundred times.
    Second, SRAM does not require any special electrical components and can be etched
    in silicon with CMOS techniques. These CMOS techniques allow FPGAs to benefit from
    all the production advancements made for modern CPUs.

    SRAM is not without its drawbacks howerver. Every SRAM cell needs 5-6 transistors
    which is quite a lot. Second, SRAM cannot persist information if a system is powered
    off. This necessitates the need for another system of persistent storage which increases
    the complexity of the system. Persistent storage is often provided by flash or EEPROM.
    Finally, storing a design in a persistent storage system like flash or EEPROM also
    makes it far easier to competitors of a company to reverse engineer a design.
    To mitigate this risk encryption is used.

    An alternative to SRAM is the so called floating gate technology [1]. This is used
    in flash or EEPROM which do not lose the data stored in them when power is lost.
    This flash based setup offers a number of benefits besides persistence.
    Persistence removes the need for a seperate storage mechanism. Persistence also
    makes it possible to run the device immediately after it starts up because there
    is no need for the programming step. This is important for certain use cases.
    Additionally, flash boasts greater area-efficiency compared to SRAM.

    One disadvantage brought by flash is that the floating gate requires that
    charge injection needs to be prevented. Another more serious downside of flash
    is its limited lifespan because can only be reprogrammed a fixed number of times.
    This is in contrast to SRAM which can be reprogrammed a virtually infinite amount of times.
    Depending on the application, this can be a serious disadvantage or a non-issue.
    The "Actel ProASIC3" for example can only be reprogrammed 500 times.
    This definitely could be an issue at least for prototyping where dozens of reprogramming
    cycles can occur in a day. Another very serious issue with flash where SRAM
    is superior is in the production run. One previously mentioned benefit of SRAM
    is that it can be produced using standard CMOS techniques. This is not the case with
    flash. Flash requires specialized components can cannot be created by etching
    silicon like with CMOS.

    In modern FPGAs there is a trend towards using a combination of flash storage and SRAM.
    This has the benefit of allowing infinite reconfigurability but this comes at a price
    of greater area overhead.

    The final type of FPGA configuration systems that are common today are anti-fuses [1].
    The most glaring difference between anti fuses and flash and SRAM is that the former
    can only be programmed once. This of course makes it unsuitable for many FPGAs applications
    like ASIC prototyping where multiple configuration runs are a neccesity.
    The name anti-fuse comes from the idea that the FPGA fabric consists of a number of fuses
    that can be selectively "blown". Though a more accurate description of this process
    would be the idea a of connecting these "fuses". This connection occurs when high voltage
    is sent through the "fuse". Most modern anti-fuses are mental-to-metal based.

    While the fact that anti-fuses cannot be reprogrammed after the initial configuration,
    they do come with the benefit of greater area efficiency [1]. In metal-to-metal
    anti-fuses this is because no silicon area is expended to allow configurable connectivity.
    One thing that does consume a significant amount of area however is the need for
    programming transistors.

    Another benefit of anti-fuses is the ability to include a greater number of switches
    because of lower on resistances and parasitic capacitence. Also,
    another obvious benefit that is shared with flash base FPGAs is the ability to
    work instantly when powered on since the fabric cannot change once programmed.

    Yet another benefit shared with flash based systems is that the design
    of the FPGA is harder to reverse engineer because it is not stored anywhere except
    the logic configuration itself. The security of the FPGA also benefits from the
    fact that because the FPGA can only be programmed once a malicious actor
    cannot reprogram the FPGA if given access to the device.

    When it comes to the manufacturing process, however, anti fuse FPGAs cannot
    benefit from using a CMOS silicon etching process [1]. Even worse,
    the fabrication process of anti fuse FPGAs is beginning to run into scaling problems
    and is generally lagging behind the sophistication of modern CMOS.

    To summarize, all three of these memory systems have their own pros and cons and
    which memory system will be used needs to be considered in light of the application
    and its constraints and goals. SRAM (as well as the flash SRAM hybrid approach) can
    be considered to be dominant howerver. This can be attributed to its compatibility
    with the sophisticated CMOS manufacturing process.
    
    \subsection{Logic Blocks}

    The basic unit of computation and storage within an FPGA is known as a logic block [1].
    The smallest a single logic block can be is to take the form of a single transistor.
    Though logic blocks can be far larger than that and even take the form of an entire
    CPU in theory. The size of a logic block, however, is something that needs to be carefully
    balanced for maxium efficiency to be achieved.

    If a logic block is too small for example, it will suffer from area inefficiency because
    it will increase the need for programmable routing. Furthermore, performance, and
    power consumption will also be negatively impacted.

    On the other extreme, logic blocks that encapsulate a large amount of logic may
    be performant, but they nullify the benefits an FPGA provides in the form of programmable
    logic. In effect, the circuit will become less and less of an FPGA, and more of
    a standard ASIC.

    Selecting logic blocks according to their size and functionality is a key consideration
    that FPGA creators (as in people creating the FGPA itself, not a design for it) need
    to consider. There are three main factors that need to be considered when selecting logic
    block type and instance count: these are area, speed and power.

    Logic blocks in modern FGPAs fall into two main categories: normal logic blocks,
    and blocks that perform specialized computations, like addition for example.
    What specialized computations will be included need to be carefully considered
    because unused specialized logic will inevitably lead to wasted area.

    Despite FPGAs being considered a completely general purpose device,
    creators of the FPGAs will in practice need to consider what kind of designs
    will run on a target FPGA [1].

    There is one main tradeoff that needs to be considered here:
    how much functionality is encapsulated into every logic block and how many
    logic blocks total are needed.
    Increasing the functionality in a single logic block reduces to total
    number of logic blocks needed (up to a diminishing point that is).
    Though as the logic block size increases, the number of wires connecting
    it in the routing increases.

    Most modern industrial FPGAs use a heirarchical approach that uses
    clusters of LUTs and flip flops [1]. This helps control the granularity
    of logic blocks.
    This involves grouping basic logic components together and connecting
    them with a local interconnect.
    This approach is used instead of increasing LUT size.
    The benefit is that the needed routing only grows quadradically instead
    of exponentially.

    To consider how logic block size affects speed several factors need to be considered.
    As stated before, increasing the size of logic blocks decreases the number of them
    that need to be used. Fewer logic blocks being used means that less routing
    logic needs to be employed. This in turn means greater performance
    because the electrical signal needs to travel a shorter distance.

    However, this reduction in delay which is outside the logic blocks is
    accompanied by a larger delay within the logic blocks.

    When it comes to power consumption, the tradeoffs are similar to those
    with area and speed [1]. Reducing the area also reduces the power needed.

    Calculating power, area, and speed is simpler when considering only
    one size of LUT. However, a heterogenous combination of LUTs can sometimes
    be more efficient when considering the different metrics.

    In contrast to LUTs, specific purpose logic is more efficient (if its being used) [1].
    Specific purpose logic is more efficient because internally, it
    has all the benefits of an ASIC and non of the inefficiencies of programmable logic.

    A downside of specific purpose logic is that it its not being used, its
    a definite net negative on all fronts.
    
    \subsection{Routing}

    The purpose of routing in an FGPA is to provide a programmable fabric that connects
    logic blocks and IO ports. Under the hood, routing uses wires which are connected
    by programmable switches [1]. For routing it is important that a large variety
    of circuit configurations are possible. Just like with logic blocks, performance
    and power consumption are in need of consideration when it comes both to designing
    the routing and configuring it to take the shape of a design.

    "Locality" in routing refers to the level of proximity that interconnected components
    exhibit. In general, it can be said that circuits exhibit a high level of locality
    because most components are close. Of course, there is also the need for connecting
    components that are far from each other.

    There are also special types of signals which need to be available globally
    in the FPGA. [1] These include clocks and resets. These kinds of signals get
    special treatment in the form of a dedicated interconnect system. These
    interconnect systems are designed to minimize skew. This means that the
    variation or noise added to the signal as it travels is minimized.

    The main type of routing that designers need to keep in mind is the so called
    general purpose routing. This is different from the global interconnect routing
    mentioned previously.
    
    One type of general purpose routing is called global routing. Global routing
    considers the properties of a routing design on a higher level while not paying attention
    to the details. general purpose routing mainly considers the locations of routing
    channels, the number of wires in a given channel, and how various channels communicate.

    In contrast to general purpose routing, detailed routing takes into consideration
    wire length, switch counts and how wires and logic block pins connect.

    When it comes to the global architecture of an FPGAs routing there are two main styles.
    The first global routing style is the hierarchical style. In this style,
    logic blocks are separated into groups. Logic blocks contained in the same group
    are connected by wire segments. Logic blocks in different groups are connected
    by wires that traverse multiple levels of routing segments.
    The further routing channels get from logic blocks, the more wires they tend to contain.

    There are several factors that need to be considered when using a hierarchical routing
    layout. One benefit of the hierarchical routing layout is that the delay
    between logic blocks is often more predictable. Performance can also be better for
    certain types of designs.
    However, if there is a mismatch between the length of a design's wires and the
    heirarchical distribution, problems may arise. Furthermore, moving between levels
    of the heirarchy can cause a significant delay. These are some of the reasons
    why modern FPGAs largely do not make use of the hierarchical routing system.

    What modern FPGAs use now for routing is the so called island style of routing.
    Island style routing arranges logic blocks in a two dimensional array and surrounds
    them with routing resources. One key decision that the designers of the FPGA itself
    need to consider here is the width of a channel.
    There are several benefits to using the island style of routing and it is likely
    that these contributed to making it the most popular style of routing in use today.

    Since a logic block has access to a varienty of wire lengths, the most efficient length
    wire can be selected. Furthermore, minimum routing delay between logic blocks is trivial
    to estimate.

    Within the island style design, there are a variety of switch block designs that
    can be used. These include the Wilton, disjoint and universal switch block designs.
    
    \subsection{Challenges of using FPGAs}

    Despite all of the improvemnts made in the field of FPGAs, there are a number of challenges
    that need to be overcome so further development can be made [1]. As CMOS processes
    have continued to become more sophisticated, several issues have become more prominent.
    As silicon chips continue to decrease in size, so called soft errors become more and more
    of a problem. A soft error occurs when ionizing radiation corrupts some data in a circuit.
    This sort of corruption does not only occur in FPGAs. Its also known problem in
    regular computer RAM. This is actually the reason why enterprise grade RAM uses
    ECC technology to detect this sort of corruption.

    The source of the radiation itself can come from both radioactive packaging and
    from cosmic radiation.

    There are a number of ways in which soft errors can be reduced in an FPGA.
    The first mitigation is at the circuit and technology level.
    One useful circuit level change is selecting an optimal memory supply voltage.
    Another technique is to add metal capacitors to memory nodes which
    decreases their sensitivity to radiation.


    A higher level mitigation is at the system level. Here, the designers of the FPGA have added builtin checks
    to find and correct corruption [1].
    One system level mitigation periodically checks the state of configuration memory
    with its correct value. If an error is found, the FPGA will need to be reprogrammed.
    A "don't care" flag is set for resources that are not currently in use since
    corruption in used resources will not affect the FPGAs functionality.
    Another, far more expensive form of mitigation, is the so called triple modular
    redundancy. Effectively, the FPGA design is replicated three full times.
    The circuitry will then vote on what output values are correct.
    This extreme of an approach would likely not work for most everyday use cases.
    Though for things like medical equipment, or for any place that handles money this
    could be useful.
    It is also noteworthy that routing is the cause for a majority of soft errors.

    When it comes to user visible memory this can also be a source of soft errors,
    the flip flops used here are actually not as vulnerable to corruption as SRAM.
    This is because SRAM is far smaller in size than flip flops. Neverthess,
    like in enterprise grade RAM, error correction can also be used here.
    It should be noted howeve, that the error correction itself is not perfect. Some
    errors will still evade detection.

    Adding such error correction comes with a cost. This is because additional data will
    need to be stored and because of the need for special encoding and decoding circuits.

    This cost can be reduced in modern FPGAs by the use of hard circuits that
    do memory encoding. Again however, like with ECC, these error correction mechanisms
    only reduce errors and do not eliminate them entirely.

    Also, a distinction needs to be made between mere error detection and full error correction.
    Since the user's design has full control over memory, the onus falls upon the user
    to hanlde errors. This is less desireable than handling errors at the FPGAs system level.
    This is because handling errors at the system level would only need to be done once,
    at the time of the FPGAs creation. These user level checks will add additional complexity
    to each and every design that uses an FPGA.
    Again, it is also up to the user to decide on the level of error correction depending
    on their application. Some applications might not cause serious consequences if
    soft errors do occur.

    There is another source of irregularities that can occur in FPGAs. The silicon etching
    process produces products that exhibit a certain degree of variations.
    Not all of these variations cause outright errors. Some of these variations simply
    cause decreased performance or increased power consumption. During manufacturing,
    as is done with CPUs, every FPGA needs to be individually tested.

    This is another area where shrinking CMOS sizes have caused an increase in complications.
    The smaller the circuts, the greater the variations in performance and power consumption
    become.
    Not only do FPGA have variation between each other as a result of this manufacturing process,
    there also exists variation within the FPGA itself. This form of variation
    is far more problematic. This is because entire FPGAs can easily be discarded,
    whereas malformed areas within an FPGA cannot be removed. These malformed areas
    mean that the clock speed of the entire FPGA needs to be reduced. For
    a 22nm process, this variation in performance can be as much as 22.4\% [1].

    Another type of problem that affects FPGA functionality are manufactoring defects.
    These are different from the process variations mentioned above because an FGPA
    with process various may still be able to works, whereas an FPGA with defects
    may be totally unusable. In the chip manufacturing industry there is a concept of
    "yield". Yield refers the the percentage of chips in a manufacturing run that
    are usable. Chips that are not part of the yield need to be discarded. These
    discarded, unusable chips need to be factored into manufacturing costs.

    Yet again, as CMOS processes improve and chips become smaller, complications arise.
    Smaller, more complex CMOS processes generally result in manufacturing defects and therefor
    significatly lower yields.

    A possible workaround for manufactoring defects, is to still use FPGAs that have manufacturing
    defects and simply not to make use of defective areas.
    The downside of this workaround is that testing a large number of FPGA chips for compatability
    with a single design may not be economical.

    Instead of simply ignoring defective areas, another approach is to build in redundancy
    from the start. This sort of approach is common in memory devices [1]. The level of
    redundancy can either be fine grained or coarse grained. Coarse grained redundancy
    adds entire rows or columns of tiles. This course grained approach has the downside
    that a significant amount of additional routing logic becomes necessary. This
    additional routing in turn has a negative impact on performance and power consumption.
    Challenges also arise when coarse grained redundancy interacts with heterogenous
    block layouts.
    A more fine grained approach merely adds additional switches. These additional switches
    are used to bypass defective routing. One benefit of fine grained redundancy is that
    these additional switches can even be used if no defects are present.
    One challenge of the fine grained approach is that a very detailed map
    of defects is needed so place and route tools can work around them.
    This detailed map adds significant complexity to the design workflow and tooling.

    Coarse grained redundancy also has the benefit of being able to handle defects in
    routing as well logic. This is in constrast to fine grained redundancy which
    cannot deal with logic block errors. Though the fine grained approach is better
    and handling interconnect errors.

    
    \subsection{FPGA Architecture Conclusion}

    The last couple of sections illustrate that FPGAs are a complex and multifacted
    technology. The people creating the FPGAs themselves as well as the developers
    of FPGA designs needs to consider a wide range of different factors. This massive
    complexity does have an upside however. It allows solutions that are very well
    tailored to specific use cases. 
    
    
    \section{FPGA Design Workflow}

    The design flow deals with the steps needed to eventually program an FPGA
    to solve a certain problem [3]. The design flow of FPGAs is actually quite
    similar to the design flow used for CLPDs and ASICs.
    In the previous section the creation of the actual FPGA itself was discussed,
    as well as programming the FPGA. This seciton will only cover the programming
    part as well as the steps leading up to it.
    The main distinction between creating an FPGA and programming for it is that
    the FPGA creators can chose what components end up on the FPGA while programmers
    merely make use of the components provided on their target FPGA.

    \subsection{The Specification}

    While some may view a formal specification as optional, a serious project
    will most likely suffer greatly if its creators forgo a real specification [3].
    Furthermore, in a team setting, a specification helps communicate to each team
    member not only what their own role is but also how their section fits into the
    larger project. The prevents multiple engineers from designing pieces that
    do not work together.

    A specification covers a number of different implementation details.
    One very useful tool to have in a specification is block diagrams [3].
    These block diagrams can both show how a design fits into a larger system,
    as well as the internal design of the FPGA. An internal block diagram shows
    how major components within the FPGA exist and are connected to one another.

    Another important piece of content is a description of what input and outputs
    will connect the FPGA to the outsie world. Another important concept that
    needs to be documented when it comes to I/O are timing estimates.

    The timing estimate for input pins covers setup and hold times.
    Output pins on the other hand need to have propagation times detailed.

    Finally there is a global clock cycle time that needs to be clearly visible.

    After the intial draft of a specification is complete, it becomes important
    to prevent the specification from growing stale by always keeping it updated.
    Its simply not possible to know every aspect of the design beforehand so
    its important that when the team decides to make changes that these changes
    are also committed to the specification.

    Part of the work of drafting a specification deals with selecting what major components
    the design will have. Weighing the positives and negatives of various FPGA products
    is one of the major parts of this process. There are numerous factors that need to be
    considered. These include the cost of the parts, their performance, their compatability
    with each other, among other factor. Another dominant factor may be
    the level of experience members of the team have with different vendors and product lines.

    Another incredibly important step in the designing phase is selecting a design
    entry method [3]. Small and less complex designs may simply use a schematic entry.
    In a schematic entry all of the routing is done manually.
    This approach has the upside of designers getting a very fine degree of control over
    how the FPGA is configured.
    There is however, a very serious downside to such and approach and that is that it
    is not viable for any even somewhat sophisticated design.

    The two technologies that are by far the most common for design entry are the languages
    Verilog and VHDL. These languages are highly portable in that they abstract
    over the low level differences that exist between FGPA models.
    They are also far more readable, flexible and expressive. These languages
    allow for what is called "synthesis" in which a software program is read
    and a low level logic gate design is generated from it.

    While these languages may share some similarities with mainstrea programming languages
    like Java, such as a shared concept of a "for" loop, they are in fact quite different.
    For starters, these languages make a distiction between logic connections that
    always exist, as well as synchronous changes that occur every clock cycle.

    Another large difference between HDLs and high level programming languages is that
    they are mostly built on the concept of wires and what connections occur between these
    wires. There have been some experimental research projects like "Kiwi" [1] that
    attempted to allow compilation from standard high level languages like C to HDLs.
    These however, have not gained mainstream traction in industry as well as for any serious
    projects in general. It seems there is just too much of a mismatch between
    these systems that prevents elegant abstractions from being created.

    After an HDL has been selected, the next step in the design process is to choose a
    synthesis tool. The purpose of the synthesis tool is to generate a logic gate
    from an HDL. Various different synthesis tools exist, but one tool that
    is particularly notable is the tool that is included in the Icestorm project.
    The Icestorm project is notable because it is completely open source, something
    which is quite rare in the hardware design industry in general.

    Following the selection of a synthesis tool, the next step in the process
    is designing a chip [3].

    \subsection{Simulation and Testing}

    While chip design is ongoing, it is important to be also meanwhile running simulations.
    Even smaller components should be tested using simulation. A lack of simulations
    can become a serious issue due to the rise of bugs that can be hard to trace if
    the design becomes a huge black box. Another great benefit of simulation is that
    in addition to getting a design running correctly, simulations are very useful
    as a form of documentation. If a developer begins working a section they know nothing
    about, simulations can serve as a reference of what the design is supposed to do.
    Simulations can even be useful to the initial developer of a component if they design
    something and come back to it weeks or months later.
    Another key role of simulation is to speed up the workflow. Since simulations
    are running regular CPUs and not on FPGAs themselves, they allow for HDLs to
    be recompiled far fasters. This is critical to the workflow, since depending on
    the complexity of the design, spending minutes or even hours resynthesizing for
    every little change can seriously hamper developer productivity.
    Not only is simlation faster, it also allows developers to have complete knowledge
    of the state of a system. This means that developers can inspect the states
    of wires that are internal to the design instead of just being able to inspect the
    outputs.

    However, after developers are satisfied with the design and it has been well tested
    in simulation, a synthesis step indeed becomes necessary.
    During synthesis, the high level hardware description language needs to be converted
    to an FPGA configuration. Part of this process involves performing place and route
    calculations.

    Another key step in converting HDL code into a real FPGA design is timing analysis.
    Timing analysis ensures that the electrical signals that travel through the FPGA
    have enough time to propagate. Without proper timing analysis, serious errors
    can occur due to the FPGA running on too high a clock speed. Once timing analysis
    is complete, designers will be given a maximum clock speed.
    Some FPGA tools, like the Icestorm project, will prevent the FPGA from being
    programmed with too high a clock speed.
    
    After a design is generated and the maximum clock speed has been found,
    it is finally time to program the FPGA.

    Once the design is programmed onto the the FPGA, manual tests can be performed
    to ensure the FPGA outputs the correct values for a given group of input values.

    Or if the FPGA is part of a larger circuit, the testing step involves ensuring
    that the complete system works as intended. If any problems are discovered,
    the workflow repeats itself and additional changes to the HDL as well as
    the simulation become necessary. Also, known bugs and other issues that were found
    should be added to the specification.

    When everything works as intended, the system will be ready to be put into production.
    A burn in test is used to ensure that a system keeps working over a long amount of time.
    However, even if a system was perfectly designed, electrical and mechanical issues
    can still arise and that is why burn in tests are useful.

    \subsection{Potential Design Problems}

    One serious design problem that may occur is race conditions [3].
    A race condition can occur if two signals which affect a given output
    can be triggered in a non deterministic order. Race conditions are dangerous
    because they depend on miniscule delays caused by variations in interal timing.
    Even a tiny change in voltage or temperatue can cause a change in the output.

    This can become a very serous problem because chips that were working perfectly
    may suddenly exhibit erroneous output after weeks, months, or even years of
    reliable operation. One way in which race conditions can be dealt with
    is to introduce a delay by converting an asynchronous operation to a synchronous one.
    However there really isn't a magic fix to eliminate all race conditions since
    they depend on a variety of complex, interwoven states.

    Another serious problems that can arise in an FPGA design is hold time violations [3].
    They are similar to race conditions in that they exist because of a timing problem.
    A hold time violation occurs if data changes at the exact same time as a clock edge.
    This means the resulting value is non-deterministic and can go either way.

    Yet another design problem caused by inconsistent timing is glitches. These occur
    when an output goes high for a very short amount of time. Unlike race conditions
    and hold time violations, eliminating glitches is more straighforward.
    To eliminate a glitch an output can be synchronized by sending it through a flip flop.

    "Metastability" is another design problem. Metastability occurs when a asynchronous
    signal is fed into a synchronous flip flop. In essence, the problem occurs because
    an asynchronous part of the design is interacting with a synchronous part.
    The fix occurs by correctly synchronizing an asynchronous signal to a given clock.
    However, there is no easy fix for metastability. Use of a synchronizer flip flop
    is a partial solution but there is still the danger of a very small chance
    that the flip flop will not resume a valid logic level. The danger of this occuring
    increases with high clock frequences. Some FPGA manufacturers include special
    synchronizer flip flops for exactly the purpose of mitigating this problem.
    Another way to decrease the change a synchronizer flip flop will not work is
    to simply include more of them.

    In general, asynchronous design is prone to a greater number of problems
    than synchronous design. This is because small, hard to debug timing errors
    can arise in asynchronous design.
    In synchronous design the delay is controlled by flip flop that are attached
    to a single clock.
    All of the problems discussed previously are because of asynchronous design
    and can be fixed by using a synchronous style.

    \section{History of Cryptology}

    Cryptology is the study of secret writing [22]. This study of secret writing
    can be broken down into two main disciplines.
    The first of these, "cryptography", concerns itself with techniques
    for creating secret writing. In a way, cryptography represents what could
    be considered the defensive side. It seeks to aid people in
    protecting secrets.
    Cryptanalysis on the other hand, also deals with finding ways of breaking
    crytography. This is the offensive side of this kind of research.

    Throughout the history of cryptology, which spans over 2 millenia,
    a wide range of methods were created with goal of hiding sensitive
    messages.

    On of the earliest recorded uses of cryptography starts with the ancient Greeks [22].
    Polybius, a Greek historian, developed a monoalphabetic substitution cipher. It
    was so influencial that the techniques it used influced cryptography two thousand
    years after its creation.
    When creating the cipher, Polybius' goal was initially not even to hide a message.
    He started with the intention of creating a system that could allow signal fires
    to communicate of large distances. In this system, every letter was represented
    by two numbers. It had the downside of having a ciphertext that was twice as long
    as necessary, though some inefficiencies could be expected given that this
    was one a very early variant.

    A few hundred years after Polybius the Roman empire had risen to prominence.
    The famous Julius Caesar himself actually was recorded as having used cryptography
    in his miliary campaigns in Gaul [22].

    Though this is not Caesars only use of cryptography during his lifetime.
    The chipher he used here was fairly straightforward. It takes the letters A-Z
    and randomly reorders them. This is the first time a monoalphabetic substitution
    cipher with a shifted alphabet was used.
    Caesar used this cipher to communicate confidentially with people loyal to him.

    After the decline of Rome, cryptography become less and less used in the western
    world for some time.
    This decline also coincided with downtrend of literacy and general scholarly activity.

    Though in the Arab world in the middle ages there is some recorded use of cryptography.
    For nearly a thousand years, the monoalphabetic substitution cipher was one
    of the strongest ciphers known. Though in the Arab world a ciphe would be developed
    that finally improved upon the status quo.

    During the ninth century AD, in a period also known as the Islamic Golden Age,
    there was a polymath scholar named Al-Kindi. Al-Kindi, being a polymath, was
    well versed in various scientific disciplines which included astronomy, philosophy
    and medicine amoung other fields. He also authored a book intended for
    secretaries in royal counts which showed how secret messages can be used.

    As stated before, after the Romans, there was a significant decline in
    virtually all fields of scholarship in the western world, specifically Europe.

    This changed with the start of the Renaissance in the 13th century.

    During this period, there lived a Franciscan monk named Roger Bacon.
    Though he lived in monasteries for most of his life, he did a lot of writing
    and experimenting.
    One of his most significant scholarly achievements was the translation
    of various Arabic texts on science and mathematics.
    
    He did not write a letter on cyprography but instead included various
    cryptographic techiques in a letter for William of Paris.

    Seven techniques were described by him in total.
   
    His philosophy about cryptography was that it was good for keeping secrets
    from the ignorant or uneducated.

    Around this time there was another user of cryptography who was arguably
    far more influencial. This influencial figure was none other than Geoffrey Chaucer.
    Chaucer was a poet who also dabbled in astronomy. He makes use of encryption
    in one of his astronomical books in sections where the usage of some atronomical tools
    is described.
    The cipher Chaucer used was the monoalphabetic substitution cihper which was hundreds
    of years old by this point.

    So far, all the people described here where only mentioned working with cyprography.
    That is, they only dealt with the protection of a secret. In this section the development
    of cryptanalysis, which deals with ways of breaking cryptography, will be explored.

    One of the first cryptoanalytic tools that was created was frequency analysis.
    Frequency analysis allowed ciphers to be broken by examining the frequency of
    encrypted letters. Some letters, such as the letter "a", are simply more common
    in language in general.
    The polymath Al-Kindi discovered this technique. He noted that, in monoalphabetic
    substition cipher, simply substituting one letter for another does nothing
    to obscure the frequency of that letter.

    This discovery by Al-Kindi was a major step forward in the field of cryptanalysis.

    Another variant of frequency analysis counts not only single letters, but
    pairs of letters. These pairs, also known as "digraphs", are even more powerful
    in breaking monoalphabetic substition ciphers.

    It wasn't until the 16th century however that cryptology began to gain
    mainstream appeal.
    Cryptology started to be used in both the military as well as in commercial applications.
    It was also in this time that a cipher that was far stronger was created.
    This far more advanced cipher would remain virtually unbreakable for hundreds of years.
    It was called the polyalphabetic substition cipher.

    Another important cipher was the biliteral cipher. This cipher was actually
    making use of stenographical techniques. This cipher was created
    by Sir Francis Bacon. Bacon was actually not only a scholar, but also quite
    involved in government. His exploits gained him quite a significant following.

    What was significant about Bacon's biliteral cipher, was that it was resistant to
    freqency analysis. It is likely that Bacon was aware of frequency analysis,
    which had become quite well known during his time, and designed his cipher
    specifically to be unbreakable.

    Where stenography comes into play here is in the encoding of an important message
    inside a fake, meaningless messsage. Furthermore, the meaningless message is
    a couple times longer than the actual message.

    According to some followers of Francis Bacon, Bacon used cyprtographic techniques
    to encode a message in plays. They also argue that Shakespears plays where
    not in fact written by Shakespear, but by Bacon. This is what is known
    as the "Baconian Theory".

    Another significant development in the cryptology world during this time
    was the use of nomenclators. Frequency analysis was a well known weakness
    of monoalphabetic substition at the time.
    The basic idea was the addition of homophones as well as a code book.
    This technique for mitigating frequency analysis did prove quite useful.

    The first nomenclator was created by Gabriele di Lavinde in 1379 in Italy.
    He created this technique for the antipope Clement VII.
    It takes a regular monoalphabetic substition cipher and combines it with a
    small code book.

    Improvements on this techniques that were made later did not use a codebook
    but homophonic substition.

    There are several downsides of using nomenclators though. The main
    downside was that despite frequency analysis becoming more difficult,
    it was still somewhat effective here.
    
    Also, all parties that wanted to encrypt or decrypt a message needed
    to have a codebook. This codebook could be found by an adversary and
    used to break the encryption.

    Despite suffering from all these downsides, nomenclators became popular
    for diplomatic, and to a lesser extent, military applications.
    As the popularity of encryption grew, the demand for cryptanalysis grew
    as well.
    This led to the rise of so called "Black Chambers". These had the purpose
    of breaking cryptography of rival or enemy factions.

    One elite Black Chamber served the Vatican.

    With dedicated code breakers in the form of Black Chambers
    growing in popularity, the arms race
    between code authors and code breakers continued.

    Cryptoanalysis had begun to gain an upper hand over cryptography,
    meaning that it became harder and harder to make unbreakable
    encryption.
    This created a need for stronger cryptography.
    Two main methods were created to strengthen cryptography.

    These were the so called "modern code", as well as the polyalphabetic substitution
    cipher.
    The monoalphabetic substition cipher, the cipher used by Julius Caesar all those
    years ago, could not stand against frequency analysis. This was
    because it simply did not obscure the original text enough
    which made it too easy to observe characteristics about the text
    despite it being encrypted. Something that helped a little bit
    in obsuring the original text was simply the removal of spaces, periods, commas,
    and so on.
    This still is not enough to hide letter frequencies however.
    A better way to obsuring letter frequencies is to use multiple cipher alphabets.
    This means that a letter can be mapped to various different letters. 

    \section{Related Work}

    Todo - add this

    THIS COMES NEAR THE END

    \break
    \section*{References}

    \begin{enumerate}

    \item Chandrasekaran, Shrutisagar, and Abbes Amira. "High performance FPGA implementation of the Mersenne Twister." 4th IEEE International Symposium on Electronic Design, Test and Applications (delta 2008). IEEE, 2008.

    \end{enumerate}
    
\end{document}
